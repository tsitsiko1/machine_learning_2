---
title: "Naïve Bayes Classifier"
output:
  html_document: default
  html_notebook: default
---
```{r, message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
packages = c("kableExtra","devtools","e1071", "caret", "ROCR", "pROC", "knitr", "tm", "SnowballC")

## Now load or install&load all
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)

devtools::install_github("haozhu233/kableExtra") # for the visualization of the tables
```

# Introduction
Welcome to the first application of Probabilistic classification with Naïve Bayes!

<html>
<img src="https://www.sketchappsources.com/resources/source-image/twitterlogo_1x.png" width="20%">
</html>

# Exercise #1
In this example we will use **Twitter** as our data source to filter those tweets that are talking about a given application. This is a typical problem in probabilistic classification, where I'll use a large sample of texts corresponding to the category that I want to recognize, and another large sample of texts unrelated to that category. That way, by exploring the different word frequencies and probabilities, we'll determine if a new text belongs to one or another category, by simply looking at the existing evidence.

## Problem Statement

There is a real app called "Mandrill"

<html>
<img src="https://pbs.twimg.com/profile_images/604326524976680960/V0gyyhdH.png" width="15%"><P>
</html>

And I want to scan twitter to capture only those tweets that mention my APP. But I don't want to read tweets talking about the animal (the actual mandrill), so I need a classifier for the tweets, that will **filter** only those which are relevant.

For this part of the problem part of the data preparation job is already done, so you start with a few hundreds tweets captured using Twitter API, with the word **Mandrill** in them. The file with tweets (`appWords.txt`) referring to the app looks like this:

    @ericcandino they're unfortunately not for sale but drop us a line via http://help.mandrill.com  a
    @gidogeek you can see what we've been working on and get a general idea of our plans at http://blo
    @guillaumepotier there are several reasons emails go to spam mind submitting a request at http://h
    @icntmx yep  we'd be glad to would you mind submitting a request at http://help.mandrill.com
    @jeremyweir if you submit a request at http://help.mandrill.com   we'll get back to you with some
    @josscrowcroft mind submitting a request via http://help.mandrill.com  with some additional detail

And the file with tweets (`otherWords.txt`) not talking about the app look like this:

    anyway  yeah  that's a thing that's going on  reincarnated mandrill-men
    arin did the spark mandrill trick i was wondering if he would :')
    audio mandrill - happy beat this is a funk song by a band who liked to w
    cannot believe i am the only one in a @mandrill 2012 #tweetfleet t-shirt
    chill penguin and spark mandrill down #megamanx
    cuando pase el bafici y se hayan perdido mandrill  mirageman  mujer metr
    de los creadores de #kiltro #mirageman y #mandrill ahora atacan con #trá

I trimmed lines for better representation, but they're arbitrarily long (within twitter limits).

As you might probably have realized, this is a **supervised problem**, and the _labeling_ of the training data has been already done, by manually separating the tweets among the two possible sets. That is the most boring part, and you always need to do so to train any classifier.

## Data Preparation

What I did to prepare the problem is to process the tweets to convert _raw_ two data files with the frequency count for each individual word on them. So, from `appWords.txt`, I generated `appFreqs.csv`, which summary is like:

```{r, echo=FALSE}
appFile <- read.csv("data/appFreqs.csv", header=F)
otherFile <- read.csv("data/otherFreqs.csv", header=F)
appTotal <- sum(appFile$V2)
otherTotal <- sum(otherFile$V2)
appFreqs <- cbind(appFile, freq=log((appFile$V2/appTotal)))
otherFreqs <- cbind(otherFile, freq=log((otherFile$V2/otherTotal)))
```

```{r,echo=F,results='asis',error=F,warning=F}
options(knitr.table.format = "html") 
kable(head(appFreqs[,1:3]), format = "html") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

And for the tweets about anything **but** the app, the result file `otherFreqs.csv` looks like this:

```{r,echo=F,results='asis',error=F,warning=F}
options(knitr.table.format = "html") 
kable(head(otherFreqs[,1:3]), format = "html") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

What I did is simply counting the number of occurrences of each word (`V1`) and put that in the column variable `V2`, but I also computed the $log$ of the probability. Remember the we can use the actual probability as:

$$ P(word) = \frac{count(word)}{\sum_{i=1}^{N}count(word_{i})} $$
or the $log(P)$, as it is more convenient to use those values than the tiny ones that the probability produces. Remember that when using $logs$ we must sum them, instead of multiplying them. So, what we have in the variable `freq` is:

$$ freq = log \left( \frac{count(word)}{\sum_{i=1}^{N}count(word_{i})} \right)  $$

### Code

To read the CSV files with the frequencies and compute the $log$ mentioned above, I used this snippet of code 

```{r, eval=FALSE, include=TRUE}
appFile <- read.csv("data/appFreqs.csv", header=F)
otherFile <- read.csv("data/otherFreqs.csv", header=F)
appTotal <- sum(appFile$V2)
otherTotal <- sum(otherFile$V2)
appFreqs <- cbind(appFile, freq=log((appFile$V2/appTotal)))
otherFreqs <- cbind(otherFile, freq=log((otherFile$V2/otherTotal)))
```

### A glimpse to the data

We will not need feature engineering, except for all the data preparation mentioned earlier. Nevertheless, let's take a look at the frequencies obtained for the different sets in the variable `V2`:

```{r, echo=FALSE}
density.app = density(appFreqs$V2, bw=10)
density.other = density(otherFreqs$V2, bw=10)
plot(density.other, xlim=c(0,200), main="Density plots for words frequency in app-related tweets (blue)\n and non-app related tweets (red)"); polygon(density.other, col=rgb(1,0,0,0.35), border="red")
lines(density.app, xlim=c(0,200)); polygon(density.app, col=rgb(0,0,1,0.35), border="blue")
```

As you can see the words frequencies obtained for the two sets are quite similar. This doesn't mean that it will be impossible to differentiate one class from the other. This simply means that the frequencies in both sets correspond to a similar communication pattern (tweets in English, mostly). Actually, I cropped the $X$ axis at 200 but they're around 3000 different words on each set, but the frequencies of the long-tail part are really small.

To classify between the two possibilities, we need to look at the words present in the new tweets, and see where are more frequent among the two distributions. Let's go for it.

#### Helper functions

I need a function gives me a word frequency in any of the data frames that I used for the two classes.

```{r}
freq  <- function(word, frame) {
  val <- frame[which(frame$V1 == word),]$freq  
  if(length(val) == 0) 1/log(sum(frame$V2))
  else val
}
```

I need to compute the prior probability of each class:

```{r}
appPrior = log(length(appFile) / (length(appFile) + length(otherFile))) # Number of Tweets in app File / Total number of Tweets (sum of the number of tweets in both files)
otherPrior = log(length(otherFile) / (length(appFile) + length(otherFile))) # Number of Tweets in other File / Total number of Tweets (sum of the number of tweets in both files)
```

### The Bayesian classifier. 

Let's build the classifier. I'm using a test set with a few tweets (`test.csv`), and the goal is to read them and say if they are about the app or not. The test set is already labeled with the class each belongs to in the first column. We will loose that information to check if our prediction is OK.

Read a test file, with the category label in V1 and the tweet contents in V2.

```{r}
test <- read.csv("data/test.csv", header=F)
pred <- character(nrow(test)) # Allocate a prediction vector
```

Now, let's loop through the file to compute the MAP (maximum A Posterior Probability) and thus, determine which class the tweet belongs to:


```{r}
# Loop the rows in the test file.
for(j in 1:nrow(test)) 
{
  tweet <- as.character(test[j,2])                 # Extract the content of the tweet
  wordsInThisTweet <- strsplit(tweet, " ")[[1]]    # Extract the words into a list.
  appProb = as.double(0.0)
  otherProb = as.double(0.0)

  # For every word in this tweet, sum its frequency value.
  for(word in wordsInThisTweet) {
    appProb   <- sum(appProb,   freq(as.character(word), appFreqs))
    otherProb <- sum(otherProb, freq(as.character(word), otherFreqs))
  }
  
  posteriorAppPob = appProb * appPrior
  posteriorOtherPob = otherProb * otherPrior
  
  # Categorize according to the score obtained from every subset (App tweets, and Other tweets)
  if(posteriorAppPob > posteriorOtherPob) {
    pred[j] <- "APP"
  } else {
    pred[j] <- "OTHER"
  }
}
```

Now print the Confusion Matrix:

```{r, echo=FALSE}
# Place the prediction vector inside the test data frame
test <- cbind(test, pred=pred)
cm <- table(test$pred, test$V1)
cm
```

An amazing accuracy in classifying tweets!




# Exercise #2
We are going to play now with another application of Naive Bayes: positive/negative prediction.

## Problem Statement
We will apply Naive Bayes to classify movie reviews according to their overall sentiment (positive/negative). In particular, we will use the Pang and Lee’s IMDB movie reviews data which contains 2000 reviews, each with a positive or negative sentiment label.

As in the previous exercise, we will make use of the bag of words representation provided by the TextMining package tm and will train a multinomial Naive Bayes classifier using and the e1071 package.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Needed <- c("tm", "SnowballCC")
#install.packages(Needed, dependencies=TRUE)
#install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")

```

## Data Preparation


```{r, echo=FALSE, message=FALSE, warning=FALSE}
dataset <- read.csv("data/movie-pang02.csv", stringsAsFactors = FALSE)

# Randomize the dataset to facilitate the training process
set.seed(123)
dataset <- dataset[sample(nrow(dataset)), ]
dataset <- dataset[sample(nrow(dataset)), ]

# Convert the target variable ('class') from character to factor.
dataset$class <- as.factor(dataset$class)
```

First thing to do is reading the files.

```{r, echo=TRUE}
corpus <- Corpus(VectorSource(dataset$text))

# We can take a look to the corpus
inspect(corpus[1:2])
```


This function cleans up a corpus (samples). The methods we decided to apply are:

  * stemming
  * remove punctuation
  * strip blanks
  * remove stopwords

```{r}
cleanCorpus <- function(corpus) {
  corpus <-tm_map(corpus, stemDocument)
  corpus.tmp <- tm_map(corpus,removePunctuation)
  corpus.tmp <- tm_map(corpus.tmp,stripWhitespace)
  corpus.tmp <- tm_map(corpus.tmp,removeWords,stopwords("en"))
  return(corpus.tmp)
}
```

Using this function, clean the corpus


```{r}
corpus.clean <- cleanCorpus(corpus)
inspect(corpus.clean[1:2])
```


Represent the bag of words tokens with a document term matrix (DTM). The rows of the DTM will correspond to the documents in the collection, columns to the terms, and its elements are the term frequencies.
```{r}
dtm <- DocumentTermMatrix(corpus.clean)

```


### Training and Test data sets.
Separate data into training and test sets (80% for training and 20% for test).

```{r}
dataset.train <- dataset[1:1500,]
dataset.test <- dataset[1501:2000,]

dtm.train <- dtm[1:1500,]
dtm.test <- dtm[1501:2000,]

corpus.clean.train <- corpus.clean[1:1500]
corpus.clean.test <- corpus.clean[1501:2000]
```

### Feature Selection
The term-frequency matrix contains more than 25.000 features (i.e., terms). As you might imagine, not all of them are useful for classification.

```{r}
dim(dtm.train)
```

Take a look to the removeSparseTerms function in the tm package and remove the sparse terms in the matrix (i.e., those terms only appearing in a few reviews)

```{r}
dtm.train.nb <- removeSparseTerms(dtm.train, 0.99)
```

Apply the same procedure for test set
```{r}
dtm.test.nb <- removeSparseTerms(dtm.test, 0.99)

```

For sentiment classification, word occurrence matters more than word frequency. Therefore, by using the following method, replace term frequencies in your dataset by Boolean presence/absence features.
```{r}
# Convert the word frequency to binary 1/0 presence
binarize <- function(dataset) {
  factor(ifelse(dataset > 0, 1,0), levels=c(0,1), labels=c("Yes", "No"))
}

# Apply the function to training and test data
dtm.train.nb.binary <- apply(dtm.train.nb, 2, binarize)
dtm.test.nb.binary <- apply(dtm.test.nb, 2, binarize)
```


### Naïve Bayes Model
Making use of the `naiveBayes` package, predict the polarity of the reviews in the test set.


```{r}
nb_model <- naiveBayes(dtm.train.nb.binary, dataset.train$class, laplace = 1)
probs <- predict(nb_model, newdata=dtm.test.nb.binary, type = "raw")
classes <- predict(nb_model, newdata=dtm.test.nb.binary, type = "class")
```

### Model evaluation

#### Confusion Matrix
Calculate the confusion matrix and the accuracy of the model. 
<b>Tip</b>: confusion matrix need the predictions as classes. Take a look to the “type” parameter in the `predict` function.

```{r}
# Confusion matrix
table("Predictions"= classes,  "Actual" = dataset.test$class )
```


Using the confusion matrix, compute the Accuracy

```{r}
acc <- function(table){
  TP = table[1,1];  # true positives
  TN = table[2,2];  # true negatives
  FP = table[1,2];  # false positives
  FN = table[2,1];  # false negatives
  acc = (TP + TN)/(TP + TN + FP + FN)
  return(acc)
}
acc(table("Predictions"= classes,  "Actual" = dataset.test$class ))
```
Almost 80 % of accuracy. Not bad for our basic and “naive” classifier.


#### ROC Curve
Compute the ROC curve (remember the `ROCR` library) and select the best probability threshold (i.e., the threshold given to the predict method that set the confidence value to assign a prediction to the positive or negative class) based on the AUC.

```{r}
pred = prediction(probs[,2], dataset.test$class)
perf <- performance(pred, "tpr", "fpr") 
plot(perf, type="b", colorize=T)

```

